---
layout: post
title: "Lecture 10"
description: ""
category: 
tags: []
---
{% include JB/setup %}

<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<a href="https://github.com/emchristiansen/CSE202/tree/gh-pages/_posts">
  <img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png" alt="Fork me on GitHub">
</a>

<!--EDIT BELOW THIS LINE, UNLESS YOU ARE DOING SOMETHING SPECIAL.-->

#Divide and conquer

  * integer multiplication
  * polynomial multiplication (FFT)
  * Master Theorem and its limitations

Consider a recurrence relation like the following.
\\[
T(n) = a T(n / b) + O(n^k)
\\]
It says your recursive solution to the problem has the structure of a tree with \\( a \\) branches per node, where each child corresponds to a subproblem of size \\( n / b \\), and where there is \\( n^k \\) work to be done at each node at this level of the tree.

Consider where the work is done in the tree (called the Master Theorem).

  * At the bottom (bottom heavy): When \\( a > b ^ k \\).
Here \\( T(n) \\) = number of leaf nodes = \\( O(a ^ D) \\), where \\( D = \log\_b n \\) is the depth of the tree.
Can be simplified to \\( O(n ^ {\log a / \log b}) \\).
  * At the top (top heavy): When \\( a < b ^ k \\).
Here \\( T(n) = O(n ^ k) \\).
  * Steady state: When \\( a = b ^ k \\).
Here \\( T(n) = O(n ^ k \log\_b n ) \\).

##Example
Suppose you have \\( n = 2^k - 1 \\) elements that you want to put in a heap.
It will be a complete binary tree of depth \\( k \\).
Can recursively solve in terms of subtrees of the heap (drawing on board).
The recurrence relation is \\( T(n) = 2 T( n / 2) + O(\log n) \\).
The Master Theorem cannot be directly applied because \\( \log n \\) is not a polynomial of \\( n \\).

So you can use a direct approach:
The cost in level 0 is \\( \log n \\).
In level 1 it is \\( 2 (\log (n / 2)) \\) which is \\( 2 (\log n - 1) \\).
Next level is \\( 4 (\log n - 2) \\).
Generally this is \\( 2 ^i (\log n - i) \\).
Sum it for \\( i = 0 \cdots \log n \\).
If you examine the series you find it converges to something \\( O(n) \\).

Another approach does use the Master Theorem.
Realize \\( 1 \leq \log n \leq \sqrt{n} \\).
Both sides of the bound lead to bottom-heavy trees, so by the Master Theorem they are \\( O (n ^ {\log a / \log b}) \\) which turns out to be \\( O(n) \\).
Since the complexity is bounded between two \\( O(n) \\) complexities, it must be \\( O(n) \\).
