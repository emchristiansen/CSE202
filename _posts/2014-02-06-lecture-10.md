---
layout: post
title: "Lecture 10"
description: ""
category: 
tags: []
---
{% include JB/setup %}

<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<a href="https://github.com/emchristiansen/CSE202/tree/gh-pages/_posts">
  <img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png" alt="Fork me on GitHub">
</a>

<!--EDIT BELOW THIS LINE, UNLESS YOU ARE DOING SOMETHING SPECIAL.-->

#Divide and conquer

  * integer multiplication
  * polynomial multiplication (FFT)
  * Master Theorem and its limitations

Consider a recurrence relation like the following.
\\[
T(n) = a T(n / b) + O(n^k)
\\]
It says your recursive solution to the problem has the structure of a tree with \\( a \\) branches per node, where each child corresponds to a subproblem of size \\( n / b \\), and where there is \\( n^k \\) work to be done at each node at this level of the tree.

Consider where the work is done in the tree (called the Master Theorem).

  * At the bottom (bottom heavy): When \\( a > b ^ k \\).
Here \\( T(n) \\) = number of leaf nodes = \\( O(a ^ D) \\), where \\( D = \log\_b n \\) is the depth of the tree.
Can be simplified to \\( O(n ^ {\log a / \log b}) \\).
  * At the top (top heavy): When \\( a < b ^ k \\).
Here \\( T(n) = O(n ^ k) \\).
  * Steady state: When \\( a = b ^ k \\).
Here \\( T(n) = O(n ^ k \log\_b n ) \\).

##Example
Suppose you have \\( n = 2^k - 1 \\) elements that you want to put in a heap.
It will be a complete binary tree of depth \\( k \\).
Can recursively solve in terms of subtrees of the heap (drawing on board).
The recurrence relation is \\( T(n) = 2 T( n / 2) + O(\log n) \\).
The Master Theorem cannot be directly applied because \\( \log n \\) is not a polynomial of \\( n \\).

So you can use a direct approach:
The cost in level 0 is \\( \log n \\).
In level 1 it is \\( 2 (\log (n / 2)) \\) which is \\( 2 (\log n - 1) \\).
Next level is \\( 4 (\log n - 2) \\).
Generally this is \\( 2 ^i (\log n - i) \\).
Sum it for \\( i = 0 \cdots \log n \\).
If you examine the series you find it converges to something \\( O(n) \\).

Another approach does use the Master Theorem.
Realize \\( 1 \leq \log n \leq \sqrt{n} \\).
Both sides of the bound lead to bottom-heavy trees, so by the Master Theorem they are \\( O (n ^ {\log a / \log b}) \\) which turns out to be \\( O(n) \\).
Since the complexity is bounded between two \\( O(n) \\) complexities, it must be \\( O(n) \\).

#Integer multiplication (continued from yesterday)
We got a recurrence of the form
\\[
T(n) = 4 T (n / 2) + O(n).
\\]
So we have a complexity of \\( O(n ^ {\log 4 / \log 2 }) = O(n ^2) \\).
So we are in the bottom-heavy case and don't beat the technique we learned in grade school.
To get a faster technique, by the Master Theorem, we can try:

  * Making \\( a\\) smaller, which means making fewer recursive calls.
  * Making \\( b \\) bigger, which means each subproblem should be smaller.

Consider replacing \\( 2 ^ { n / 2 } \\) with an abstract variable \\( z \\).
So
\\[
P\_{xy}(z) = x\_h y\_h z ^ 2 + (x\_h y\_l + x\_l y\_h) z + x\_l y\_l.
\\]
To compute the values of the above polynomial, we only need to know its values at 3 points.
Evaluate at \\( 0, 1, -1 \\).
\\[
P\_{xy}(0) = x\_l y\_l
\\]
\\[
P\_{xy}(1) = (x\_h + x\_l)(y\_h + y\_l)
\\]
\\[
P\_{xy}(-1) = (-x\_h + x\_l)(-y\_h + y\_l)
\\]

The above involves 3 recursive calls.
Once you have the polynomial at the three points, you can calculate the high, middle, and low order terms using simple operations:

  * \\( M = (P\_{xy}(1) - P\_{xy}(-1)) / 2 \\)
  * \\( L = P\_{xy}(0) \\)
  * \\( H = P\_{xy}(1) - M - L \\)

The final result is \\( H 2 ^ n + M 2 ^ {n / 2} + L \\).

In the above, the recurrence relation is
\\[
T(n) = 3 T(n / 2) + O(n).
\\]
We're still in the bottom-heavy case, so the complexity is \\( O(n ^ { \log\_2 3 }) \\).
